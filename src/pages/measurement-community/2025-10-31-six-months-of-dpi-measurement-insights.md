---
layout: ../../layouts/Standard.astro
title: "Six Months of DPI Measurement Insights: What We've Learned from Our Community of Practice"
date: 2025-10-31
published: true
---

## Introduction

Digital public infrastructure holds transformative promise: enabling millions to access financial services for the first time, streamlining government service delivery, and creating digital foundations for economic growth.  
  
However, this promise alone is insufficient. As governments worldwide accelerate DPI deployment, measurement emerges as the critical mechanism for ensuring these systems deliver on their foundational commitments to inclusion, security, and public value. Without robust measurement frameworks, we risk building digital infrastructure that serves some while excluding others, that prioritises efficiency over equity, or that creates new forms of digital divide rather than bridging existing ones.  
  
Six months ago, we launched the Community of Practice on DPI Measurement with a fundamental premise: effective measurement is not merely about technical assessment but about accountability to public values. Through intensive discussions covering inclusion, interoperability, trust, security, and scope evaluation, our community has generated insights that challenge conventional approaches and point toward more sophisticated, contextually-sensitive measurement frameworks.

## The Theory-Practice Gap in DPI Measurement

Our most significant discovery concerns the persistent disconnect between theoretical measurement frameworks and implementation realities. Current approaches consistently fail to capture the complex, often messy dynamics that determine whether DPI actually works for people.

**Governance Challenges Exceed Technical Solutions**: Our security session provided the starkest illustration of this theory-practice gap. While broad consensus exists around principles like privacy by design and transparency, our community discussion revealed that "governance challenges exceed technical solutions." Regulatory frameworks, institutional mandates, and cross-agency coordination present greater barriers than technical security controls.

Traditional enterprise security models prove inadequate for DPI contexts that operate "at the intersection of technology and governance." Effective measurement requires frameworks that assess institutional capacity, regulatory alignment, and coordination mechanisms—not just system vulnerabilities. The most secure technical architecture fails if implementing institutions lack the capacity to govern it effectively.

**Standards vs. Real-World Outcomes**: Our interoperability session challenged the common practice of using standards compliance as a proxy for effective interoperability. Expert practitioners revealed that "the standards-interoperability relationship is contextual", standards become essential as systems mature, but their mere existence doesn't guarantee functional connectivity.  
  
A community member's question "Why do we need to measure DPI interoperability?" prompted recognition that measurement must assess whether interoperability achieves its intended outcomes. The academic and researcher Vanita Falcao who I collaborated with to design session Session 5: Measuring “Just” DPI described watching elderly citizens queue for hours at multiple offices to update the same information across disconnected systems, a situation which highlights the ongoing gap between the states goals and lived reality of DPI implementations. Another participant in the same session shared their experience of how entrepreneurs in rural areas abandoned digital payment systems entirely after discovering that interoperability gaps meant accepting payments from only certain providers, fragmenting their customer base and increasing transaction costs beyond what cash operations required.  
  

## Beyond Technical Metrics: The Importance of User-Centric Measurement

Measurement shapes whose stories get told. A persistent point of discussion that developed across numerous sessions was the potential dangers that emerge when measurement practices lose sight of the human lives at the heart of the DPI story. 

**Meaningful vs. Nominal Inclusion**: The distinction between counting users and understanding impact emerged as perhaps our most uncomfortable reckoning. Digital researcher and legal scholar Rafael Zanatta crystallised this tension with a deceptively simple question during our May session: "Can you log in and use [gov.br](http://gov.br) without any kind of assistance?"

The question exposed how our metrics systematically obscure digital exclusion. Systems report millions of "registered users" while remaining functionally inaccessible to those lacking digital literacy, reliable internet, or the cultural capital to navigate bureaucratic interfaces. Zanatta's intervention forced us to acknowledge what practitioners had long observed: the elderly woman counted as a "digital ID holder" who must rely on her grandson to access her pension; the rural farmer "enrolled" in digital agricultural services but unable to complete basic transactions without visiting a physical office; the immigrant worker technically "included" in the system but confronted by language barriers and interface assumptions that render services unusable.

Similar points were raised in another session when community members challenged how inclusion metrics were being used, emphasising how adoption metrics can obscure coercion: "When we say adoption, adoption was especially for welfare entitlements, coercion." Their pointed question: "If we are measuring DPI from a perspective of public trust... what would then our criteria be?", challenged us to acknowledge that adoption through coercion fundamentally differs from voluntary engagement, yet current metrics fail to capture this distinction.

**Context-Sensitive Security Priorities**: During our fourth session on security which took place in July the community tackled questions related to how historical relationships with state power fundamentally shape security concerns. One particularly salient point was raised by Matteo Rodriguez from the Global Solutions Initiative who observed that "public trust and adoption is mostly a problem in European and Western countries, as opposed to global majority contexts where it's less emphasised".  
  
These comments aren’t necessarily about different values around privacy or security, but about different lived realities. Where state surveillance historically threatens citizens, populations demand data protection. Where state absence creates the primary risk communities prioritise functional delivery over privacy safeguards.  
  
This divergence reflects profound differences in institutional histories and threat models. In contexts where state surveillance has historically threatened citizen privacy, populations prioritise data protection and consent mechanisms. Yet in settings where state absence (not overreach) poses the primary risk, communities may prioritise service reliability and access over privacy safeguards. A participant from East Africa noted that rural communities worried less about government data collection than about whether digital systems would function during the rainy season when connectivity fails.

These variations don't reflect different values around privacy or security, but different calculations about immediate versus hypothetical risks. Where traditional state capacity remains limited, the promise of any functional service delivery may outweigh abstract privacy concerns. Where strong state apparatus already exists, citizens have the luxury—and necessity—of demanding privacy protections. As Rodriguez later questioned, the challenge becomes how to "translate these technical and abstract indicators into something that the population understands and which can increase trust."

**Separating DPI Performance from Inherited Realities**: One of our most nuanced insights emerged around the challenge of attribution in DPI measurement. Community members highlighted how current approaches often conflate the effects of DPI implementations with pre-existing conditions shaped by decades of traditional infrastructure development and state capacity.

In contexts with limited historical state capacity or infrastructure reach, populations may have low expectations of government service delivery regardless of DPI quality. Conversely, in settings with strong traditional infrastructure, high user expectations may lead to criticism of DPI systems that are technically superior to legacy alternatives. This creates a fundamental measurement challenge: how do we distinguish between outcomes attributable to DPI design and implementation versus those shaped by inherited institutional contexts and population expectations?

Effective measurement frameworks must account for these baseline conditions, developing approaches that can isolate DPI-specific impacts from broader governance and infrastructure legacies. This requires sophisticated methodological approaches that establish appropriate comparison points and control for historical context when assessing DPI effectiveness.

## Building Evidence-Based Measurement Frameworks

What has struck me across these sessions was how researchers working independently across continents had converged on the same methodological insight: measurement must begin with what actually happens when people encounter DPI, not with what our frameworks assume should happen. As I’ve watched this community grow over the last 6 months it seems to me that we are witnessing the clarification of both measurement methodologies and the values that shape them. 

**Testing as Research**: Our interoperability session introduced "testing as research": creating empirical tests that directly measure system compatibility rather than relying on theoretical assessments. This approach, exemplified by practitioners testing whether elderly citizens could actually complete transactions across supposedly "interoperable" payment systems, illustrated the development of abstract standards into measurable outcomes reflecting actual user experiences.

**Implementation Experience as Primary Data**: Community discussion emphasised the importance of empirical research: "If you're really going to understand how the system is governed and performed, you have to apply research to specific projects with access to the projects and stakeholders to see how it works in practice."

This challenges measurement approaches relying primarily on desk research or theoretical analysis. Data Privacy Brazil's six-year longitudinal study of [gov.br](http://gov.br) demonstrates this principle, their discovery of AI-enabled fraud emerged only through sustained field observation, not compliance audits helping to provide transparency through measurement. Our community consistently found the most valuable insights came from practitioners sharing real-world experiences rather than academic presentations.

**Adaptive Framework Development**:  Rather than rigid measurement standards, our community advocated for adaptable approaches responding to varying institutional maturity and technical capacity while providing clear implementation guidance. To my mind, the IIPP’s ongoing work on the DPI Map embodies this principle, adjusting its measurement criteria based on whether systems are emerging, established, or mature, recognising that a rural mobile money deployment faces different constraints than Estonia's digital governance infrastructure.

The adaptive approach reflects recognition that DPI measurement cannot follow a one-size-fits-all model. Different countries have varying institutional capabilities, regulatory environments, and development priorities. Effective frameworks must be sophisticated enough to account for this variation while remaining practical for widespread adoption.

## Community Voices and Evolving Debates

The most compelling insights emerged from vigorous debates among session participants, revealing fundamental tensions in how we conceptualise DPI effectiveness.

**The Politics of Measurement**: Our trust session generated pointed debates about measurement's political dimensions. A community member's challenge to acknowledge coercion in adoption metrics sparked broader discussion about whether measurement frameworks can remain politically neutral. Their observation forced our community to confront how measurement choices reflect political assumptions about state power and citizen autonomy.

These debates revealed that measurement is never purely technical. The metrics we choose embody assumptions about what constitutes successful DPI. Our community increasingly recognised the need for approaches that make these assumptions explicit rather than hiding them behind claims of technical objectivity.

**Global Context Differences**: Multiple sessions revealed significant differences in how participants from different regions prioritise measurement concerns. While European and North American participants often emphasised privacy and consent, Global South participants more frequently highlighted service delivery effectiveness and administrative efficiency.

These differences reflect different development priorities and institutional contexts. Our community learned to approach framework development with greater contextual sensitivity, recognizing that effective frameworks must be adaptable to diverse priorities and constraints.

## Pathways Forward

Several clear pathways emerge for advancing DPI measurement practice based on our six months of intensive discussion.

**From Compliance to Outcome Measurement**: Our most consistent finding concerned the need to shift from compliance-based metrics to outcome-focused assessment. Rather than measuring whether systems meet technical specifications, effective frameworks must assess whether they achieve intended social and economic outcomes.

This shift requires developing methodologies for measuring complex social phenomena like trust, inclusion, and institutional effectiveness. It also requires building capacity among measurement practitioners for sophisticated assessments beyond technical auditing.

**Building Measurement Communities**: Our Community of Practice model demonstrates the value of collaborative framework development. Moving forward, we recommend establishing regional measurement communities that can adapt global frameworks to local contexts while contributing to knowledge sharing.

These communities should include technical experts, implementers, policymakers, civil society representatives, and affected populations. The most valuable measurement insights come from those who experience DPI systems directly.

**Investing in Empirical Research**: Effective framework development requires substantial investment in empirical research examining how DPI systems function across diverse contexts. This research must include systematic comparative analysis across implementations, understanding relationships between design choices and outcomes, and examining experiences of marginalised populations often excluded from measurement exercises.

## Join the Conversation

The insights presented here represent just the beginning of what we hope will be an ongoing evolution in how we understand and measure DPI effectiveness. Our Community of Practice continues to meet monthly, exploring emerging measurement challenges and developing practical assessment approaches.

**Get Involved**: The Community of Practice continues to evolve based on member contributions and interests. To suggest topics for future sessions or share implementation experiences that could inform measurement framework development, please reach out through our [feedback form](https://docs.google.com/forms/d/e/1FAIpQLSfSGSYL6lOwzNBqtwsiRYKQXnBGSx32IXeh9fPpWQjx2r0erg/viewform?usp=dialog).

**Stay Connected**: Subscribe to our [DPI Map Newsletter](https://docs.google.com/forms/d/e/1FAIpQLSef0ja9DQhV9uBpgSBILh0eNT152Y2nv_9DRGZNFqulZT09Eg/alreadyresponded) to receive updates about upcoming sessions and research insights.

**Upcoming Sessions**: Details about our next session will be shared shortly. We continue to welcome suggestions for future topics and speakers through our ongoing community input process.

For questions about the Community of Practice or to discuss collaboration opportunities around DPI measurement, contact our community manager Mitchel Pass at [m.pass@ucl.ac.uk](mailto:m.pass@ucl.ac.uk).

The questions and challenges raised in our sessions are shaping more nuanced approaches to DPI measurement across policy and research communities. As global DPI deployment accelerates, the need for robust measurement frameworks becomes increasingly urgent. We invite researchers, practitioners, policymakers, and civil society representatives to join us in developing measurement approaches that can ensure DPI delivers on its transformative potential while protecting public values.

_By Mitchel Pass, UCL Institute for Innovation and Public Purpose_
